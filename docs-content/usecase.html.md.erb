---
title: The Purpose of the a9s Consul DNS for PCF tile
owner: Partners
---

## <a id="why-not-proxy"></a> What is the purpose of the a9s Consul DNS for PCF tile?

The a9s Consul DNS tile provides an internal DNS system to the a9s service tile (e.g. a9s MongoDB for PCF and a9s PostgreSQL for PCF). This internal DNS system is used by the service tiles for the following reasons/purposes:

* When binding an a9s service instance (e.g. a dedicated MongoDB ReplicaSet) to an application in Cloud Foundry, a <b>hostname</b> will be passed in the credentials (instead of IP addresses). This makes the <b>service bindings independent from the IP addresses</b>. Whenever an IP address of one node in the service cluster changes, the service binding must not be recreated.
* The a9s Consul DNS for PCF tile allows to <b>realize an infrastructure independent failover</b> strategy. The a9s PostgreSQL for PCF tile for examples makes use of this. For each PostgreSQL cluster there is only one master node which acceptes write requests. The applications usually only accepts the hostname of the master node. Because its not garantueed that the node which is the master today is also the master tomorrow the a9s PostgreSQL for PCF creates dedicated hostname always resolving to the IP address of the current master. This dedicated hostname is then passed to the service bindings. When the master changes, the a9s PostgreSQL for PCF will update the IP address of the master hostname but the hostname itself dosn't change and so the service bindings doesn't have to be changed.
* The a9s Consul DNS for PCF tile allows to <b>realize a infrastructure independent load balancing</b> (based on Consuls DNS round robin capabilities). At the moment there is no a9s service in the Pivotal Network which makes use of this but the service tile components use this load balancing mechanism to distribute the load across multiple instances of a service broker.
* The a9s Consul DNS acts as a service registry for the internal DSF components. <a href="https://www.consul.io/docs/agent/watches.html" target="_blank">Consul watches</a> are used to realize a choregraphy pattern within the DSF microservice architecture.

## <a id="why-not-proxy"></a>Why we decided not to use NGINX/HAProxy

While a Proxy could solve the issue of IP propagation, those instances will introduce another layer of complexity. For a service like PostgreSQL, drivers will expect only one node to contact, which makes the proxy a single point of failure and strips most advantages in terms of HA, as the proxy will be the single point of failure. Another idea is to deploy multiple proxies within a hot-standby setup, which will add another layer of complexity, as an IaaS specific failover needs to be implemented (e.g. OpenStack does not allow transfer of Static IPs for instances on a non reachable Compute Node, except when using a Floating IP).

## <a id="why-not-ips"></a>Why we decided not to use static IP addresses

While it sounds tempting to use the IPs of the service instances, there is a major downside. Depending on the IaaS, it might not be possible to resurrect VMs with static IPs if the compute node containing it is currently down. The anynines data service framework is aiming to support all network types, this includes dynamic networks, where crashed VMs might be resurrected with a different IP.

## <a id="why-not-ips"></a>Why we are not using Cloud Foundry's TCP router

Cloud Foundrys TCP router is a great component which came up after we developed and used the anynines Data Service Framework (DSF). This framework is the core of all a9s service tiles so we already had a solution in place. However, we had some internal discussions to use the TCP router within the anynines DSF. Because we see advantages and disadvantages in both solutions (using the Consul DNS vs. using the TCP router) we consider to provide both solutions in the future.

A realization of this could provide two different types of hostnames in the service bindings. One filed ("consul_hostname" or "consul_hostnames") that contains the  hostnames wihch are resolved via the Consul DNS and one field that contains the hostnames where the traffic is routed througt the TCP router ("tcp_route" or "tcp_routes"). The tile administrator could choose which hostname type is passed in the default "host" or "hosts" field of the service binding credentials. The applications could then pick the type of hostname(s) which is most appropriate.

We did not investigated in a prototype yet but we want to share our first thoughts on that.

* Using the TCP Router for an <b>infrastructure independent load balancing</b> would also be possible.
* Using the TCP Router for an <b>infrastructure independent failover</b> would also be possible.
* Using the same TCP Router instances the PCF Elastic Runtime uses for the applications would make the service instance endpoints available on the internet. It's required to deploy dedicted TCP router instances on a private network.
* It introduces an addional hop in the network since the traffic is first routed to the TPC router and from there to the service instance. Using the Consul DNS solution the traffic is directly routed to the service instance without any other  compoent in between.
* A high available TCP Router setup requires a high available load balancer as an addioninal component. If you don't want all service instances to become available on the internet you need a high available load balancer that is not available on the internet.
* Because we want to provide dedicated service instances the TCP router would be a shared component throught which the traffic of all service instances is going through. Using a dedicated TCP routers for each service instance would also be possible. This means there could be a service plan that provisions dedicated service instances but use a shared TCP router. Another plan could provisions a dedicated a TCP router for each service instance. For the last option it is required to dynamicly setup a load balancer.
